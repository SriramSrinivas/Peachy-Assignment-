{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c6fa8d-e055-4917-9fe3-3fcf0c00ed34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (2.5.1.post303)\n",
      "Requirement already satisfied: torchvision in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (0.20.1a0+9f8010e)\n",
      "Requirement already satisfied: torchaudio in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (2.5.1a0+d6d4767)\n",
      "Requirement already satisfied: filelock in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: numpy in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from torchvision) (2.2.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from sympy!=1.13.2,>=1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu118.html\n",
      "Requirement already satisfied: torch-geometric in /home/networks/sriram88/.local/lib/python3.12/site-packages (2.6.1)\n",
      "Requirement already satisfied: torch-scatter in /home/networks/sriram88/.local/lib/python3.12/site-packages (2.1.2+pt25cu118)\n",
      "Requirement already satisfied: torch-sparse in /home/networks/sriram88/.local/lib/python3.12/site-packages (0.6.18+pt25cu118)\n",
      "Requirement already satisfied: torch-cluster in /home/networks/sriram88/.local/lib/python3.12/site-packages (1.6.3+pt25cu118)\n",
      "Requirement already satisfied: torch-spline-conv in /home/networks/sriram88/.local/lib/python3.12/site-packages (1.2.2+pt25cu118)\n",
      "Requirement already satisfied: pyg-lib in /home/networks/sriram88/.local/lib/python3.12/site-packages (0.4.0+pt25cu118)\n",
      "Requirement already satisfied: aiohttp in /home/networks/sriram88/.local/lib/python3.12/site-packages (from torch-geometric) (3.12.14)\n",
      "Requirement already satisfied: fsspec in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from torch-geometric) (2024.12.0)\n",
      "Requirement already satisfied: jinja2 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from torch-geometric) (3.1.5)\n",
      "Requirement already satisfied: numpy in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from torch-geometric) (2.2.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from torch-geometric) (6.1.1)\n",
      "Requirement already satisfied: pyparsing in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from torch-geometric) (3.2.1)\n",
      "Requirement already satisfied: requests in /home/networks/sriram88/.local/lib/python3.12/site-packages (from torch-geometric) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /home/networks/sriram88/.local/lib/python3.12/site-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: scipy in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from torch-sparse) (1.15.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from aiohttp->torch-geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from aiohttp->torch-geometric) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from aiohttp->torch-geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from aiohttp->torch-geometric) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from aiohttp->torch-geometric) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from aiohttp->torch-geometric) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from aiohttp->torch-geometric) (1.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from jinja2->torch-geometric) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from requests->torch-geometric) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from requests->torch-geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from requests->torch-geometric) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from requests->torch-geometric) (2025.7.14)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorboard in /home/networks/sriram88/.local/lib/python3.12/site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from tensorboard) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from tensorboard) (1.73.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from tensorboard) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: packaging in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from tensorboard) (24.2)\n",
      "Requirement already satisfied: pillow in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from tensorboard) (11.1.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from tensorboard) (6.31.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from tensorboard) (75.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dask in /home/networks/sriram88/.local/lib/python3.12/site-packages (2025.7.0)\n",
      "Requirement already satisfied: click>=8.1 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from dask) (8.2.1)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from dask) (3.1.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from dask) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from dask) (24.2)\n",
      "Requirement already satisfied: partd>=1.4.0 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from dask) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from dask) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from dask) (1.0.0)\n",
      "Requirement already satisfied: locket in /home/networks/sriram88/.local/lib/python3.12/site-packages (from partd>=1.4.0->dask) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dask in /home/networks/sriram88/.local/lib/python3.12/site-packages (2025.7.0)\n",
      "Requirement already satisfied: distributed in /home/networks/sriram88/.local/lib/python3.12/site-packages (2025.7.0)\n",
      "Requirement already satisfied: click>=8.1 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from dask) (8.2.1)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from dask) (3.1.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from dask) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from dask) (24.2)\n",
      "Requirement already satisfied: partd>=1.4.0 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from dask) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from dask) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from dask) (1.0.0)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from distributed) (3.1.5)\n",
      "Requirement already satisfied: locket>=1.0.0 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from distributed) (1.0.0)\n",
      "Requirement already satisfied: msgpack>=1.0.2 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from distributed) (1.1.1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from distributed) (6.1.1)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from distributed) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from distributed) (3.1.0)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from distributed) (6.4.2)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from distributed) (2.5.0)\n",
      "Requirement already satisfied: zict>=3.0.0 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from distributed) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from jinja2>=2.10.3->distributed) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyMetis in /home/networks/sriram88/.local/lib/python3.12/site-packages (2025.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from PyMetis) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch-cluster in /home/networks/sriram88/.local/lib/python3.12/site-packages (1.6.3+pt25cu118)\n",
      "Requirement already satisfied: scipy in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from torch-cluster) (1.15.1)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from scipy->torch-cluster) (2.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorboard in /home/networks/sriram88/.local/lib/python3.12/site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from tensorboard) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from tensorboard) (1.73.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from tensorboard) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: packaging in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from tensorboard) (24.2)\n",
      "Requirement already satisfied: pillow in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from tensorboard) (11.1.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from tensorboard) (6.31.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from tensorboard) (75.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/networks/sriram88/.local/lib/python3.12/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /util/opt/anaconda/deployed-conda-envs/packages/pytorch-gpu/envs/pytorch-gpu-2.5.1-py312/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv pyg-lib \\\n",
    "  -f https://data.pyg.org/whl/torch-2.5.1+cu118.html\n",
    "%pip install tensorboard\n",
    "%pip install dask\n",
    "%pip install dask distributed\n",
    "%pip install PyMetis\n",
    "%pip install torch-cluster\n",
    "%pip install tensorboard\n",
    "%pip install torch_tb_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7717831-7ddb-4183-aeee-38a9a25d0589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "else:\n",
    "    print(\"No GPUs available. PyTorch is running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3e79b-5836-4095-87c8-bc0d1ba9b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Reddit\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import cProfile # For general Python profiling (kept for Node2Vec if desired, but not for GCN training)\n",
    "import pstats # For general Python profiling stats (kept for Node2Vec)\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# Try to import Node2Vec and its dependencies\n",
    "try:\n",
    "    from torch_geometric.nn import Node2Vec # Import Node2Vec for random walk embeddings\n",
    "    NODE2VEC_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Node2Vec or its dependencies ('pyg-lib' or 'torch-cluster') not found.\")\n",
    "    print(\"Please install them for Node2Vec functionality:\")\n",
    "    print(\"pip install pyg-lib\")\n",
    "    print(\"pip install torch-cluster\")\n",
    "    NODE2VEC_AVAILABLE = False\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression # For classification on embeddings\n",
    "from sklearn.metrics import accuracy_score, adjusted_rand_score, normalized_mutual_info_score # For evaluating Node2Vec classifier and clustering\n",
    "\n",
    "from sklearn.cluster import KMeans # For clustering on GCN embeddings\n",
    "\n",
    "# Import for DistributedDataParallel\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp # Still needed for DDP process group initialization\n",
    "\n",
    "# Import for Dask\n",
    "from dask.distributed import Client, LocalCluster, get_worker\n",
    "\n",
    "# Import for Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import PyTorch Profiler\n",
    "import torch.profiler\n",
    "\n",
    "# Directory for TensorBoard logs\n",
    "LOG_DIR = \"runs/hpc_gcn_profiler\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# Try to import PyMetis. If not available, provide instructions.\n",
    "try:\n",
    "    import pymetis\n",
    "    METIS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"PyMetis not found. Please install it for METIS partitioning:\")\n",
    "    print(\"1. Install METIS library: sudo apt-get install libmetis-dev (on Ubuntu/Debian)\")\n",
    "    print(\"2. Install PyMetis: pip install PyMetis\")\n",
    "    print(\"Falling back to only random partitioning if PyMetis is not installed.\")\n",
    "    METIS_AVAILABLE = False\n",
    "\n",
    "# --- 1. Load the Dataset ---\n",
    "print(\"Loading dataset...\")\n",
    "# Using PubMed for quicker demonstration, but can be switched to Reddit.\n",
    "dataset = Planetoid(root='/tmp/PubMed', name='PubMed')\n",
    "# dataset = Reddit(root='/tmp/Reddit') # Uncomment to use Reddit dataset\n",
    "data = dataset[0]\n",
    "print(f\"Dataset loaded: {dataset.name if isinstance(dataset, Planetoid) else 'Reddit'}\") # Adjusted for Reddit dataset\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Number of features: {data.num_node_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n",
    "print(f\"Graph has isolated nodes: {data.contains_isolated_nodes()}\")\n",
    "print(f\"Graph has self-loops: {data.contains_self_loops()}\")\n",
    "print(f\"Graph is undirected: {data.is_undirected()}\")\n",
    "\n",
    "# For node classification, we typically use data.train_mask, data.val_mask, data.test_mask\n",
    "print(f\"Number of training nodes: {data.train_mask.sum()}\")\n",
    "print(f\"Number of validation nodes: {data.val_mask.sum()}\")\n",
    "print(f\"Number of test nodes: {data.test_mask.sum()}\")\n",
    "\n",
    "# --- 2. Define the GCN Model ---\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, return_embeddings=False):\n",
    "        # The first layer output can be considered as node embeddings\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        \n",
    "        if return_embeddings:\n",
    "            return h # Return embeddings from the hidden layer\n",
    "        \n",
    "        out = self.conv2(h, edge_index)\n",
    "        return out\n",
    "\n",
    "# --- 3. Graph Partitioning Functions ---\n",
    "\n",
    "def convert_to_metis_format(edge_index, num_nodes):\n",
    "    \"\"\"\n",
    "    Converts PyG edge_index to METIS adjacency list format.\n",
    "    METIS expects adjacency list where each list contains neighbors of a node.\n",
    "    \"\"\"\n",
    "    adj_list = [[] for _ in range(num_nodes)]\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        u, v = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "        adj_list[u].append(v)\n",
    "        # If graph is undirected and edge_index only contains one direction, add reverse\n",
    "        if data.is_undirected(): # Assuming data.is_undirected() is reliable\n",
    "             adj_list[v].append(u)\n",
    "    return adj_list\n",
    "\n",
    "def random_partition_graph(data, num_partitions):\n",
    "    \"\"\"\n",
    "    Randomly partitions nodes into num_partitions.\n",
    "    Returns a list of (node_mask, edge_index_for_partition) for each partition.\n",
    "    \"\"\"\n",
    "    print(f\"Performing random partitioning into {num_partitions} parts...\")\n",
    "    start_time = time.time()\n",
    "    num_nodes = data.num_nodes\n",
    "    node_partition_ids = torch.randint(0, num_partitions, (num_nodes,))\n",
    "\n",
    "    partitions = []\n",
    "    for i in range(num_partitions):\n",
    "        # Nodes belonging to this partition\n",
    "        partition_node_mask = (node_partition_ids == i)\n",
    "        \n",
    "        # Create a subgraph for this partition\n",
    "        # This is a simplification: in true distributed, you'd only pass relevant edges.\n",
    "        # Here, we're creating a 'view' of the original graph for each partition.\n",
    "        # For a more rigorous approach, you'd need to re-index nodes within each partition.\n",
    "        \n",
    "        # Filter edges where both source and destination nodes are in this partition\n",
    "        # This simulates internal edges. Edges crossing partitions are \"cut edges\".\n",
    "        src, dst = data.edge_index\n",
    "        edge_mask_internal = (node_partition_ids[src] == i) & (node_partition_ids[dst] == i)\n",
    "        partition_edge_index = data.edge_index[:, edge_mask_internal]\n",
    "\n",
    "        # Count cut edges\n",
    "        edge_mask_cut = (node_partition_ids[src] == i) & (node_partition_ids[dst] != i)\n",
    "        num_cut_edges = edge_mask_cut.sum().item()\n",
    "\n",
    "        partitions.append({\n",
    "            'node_mask': partition_node_mask,\n",
    "            'edge_index': partition_edge_index,\n",
    "            'num_cut_edges': num_cut_edges,\n",
    "            'num_nodes_in_partition': partition_node_mask.sum().item()\n",
    "        })\n",
    "    end_time = time.time()\n",
    "    print(f\"Random partitioning complete in {end_time - start_time:.4f} seconds.\")\n",
    "    return partitions\n",
    "\n",
    "def metis_partition_graph(data, num_partitions):\n",
    "    \"\"\"\n",
    "    Partitions the graph using METIS.\n",
    "    Returns a list of (node_mask, edge_index_for_partition) for each partition.\n",
    "    \"\"\"\n",
    "    if not METIS_AVAILABLE:\n",
    "        print(\"METIS is not available. Skipping METIS partitioning.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Performing METIS partitioning into {num_partitions} parts...\")\n",
    "    start_time = time.time()\n",
    "    num_nodes = data.num_nodes\n",
    "    adj_list = convert_to_metis_format(data.edge_index, num_nodes)\n",
    "\n",
    "    # PyMetis returns edge cuts and node assignments\n",
    "    edge_cuts, node_partition_ids = pymetis.part_graph(num_partitions, adj_list)\n",
    "    node_partition_ids = torch.tensor(node_partition_ids, dtype=torch.long)\n",
    "\n",
    "    partitions = []\n",
    "    total_cut_edges = 0\n",
    "    for i in range(num_partitions):\n",
    "        partition_node_mask = (node_partition_ids == i)\n",
    "        \n",
    "        src, dst = data.edge_index\n",
    "        # Edges where both source and destination nodes are in this partition\n",
    "        edge_mask_internal = (node_partition_ids[src] == i) & (node_partition_ids[dst] == i)\n",
    "        partition_edge_index = data.edge_index[:, edge_mask_internal]\n",
    "\n",
    "        # Count edges where source is in this partition and destination is not\n",
    "        edge_mask_cut_from_this_partition = (node_partition_ids[src] == i) & (node_partition_ids[dst] != i)\n",
    "        num_cut_edges_from_this_partition = edge_mask_cut_from_this_partition.sum().item()\n",
    "        total_cut_edges += num_cut_edges_from_this_partition\n",
    "\n",
    "        partitions.append({\n",
    "            'node_mask': partition_node_mask,\n",
    "            'edge_index': partition_edge_index,\n",
    "            'num_cut_edges': num_cut_edges_from_this_partition,\n",
    "            'num_nodes_in_partition': partition_node_mask.sum().item()\n",
    "        })\n",
    "    end_time = time.time()\n",
    "    print(f\"METIS partitioning complete in {end_time - start_time:.4f} seconds.\")\n",
    "    print(f\"Total METIS cut edges (reported by METIS): {edge_cuts}\")\n",
    "    # Note: The sum of num_cut_edges_from_this_partition across all partitions will be 2x the actual cut edges\n",
    "    # because each cut edge is counted twice (once from each side).\n",
    "    print(f\"Total METIS cut edges (calculated from partitions, should be 2x actual): {total_cut_edges}\")\n",
    "    return partitions\n",
    "\n",
    "# --- 4. Training Function for a Single Partition (Worker) ---\n",
    "# This function will be executed by each Dask worker.\n",
    "def train_partition_worker(rank, world_size, partition_data_list,\n",
    "                           num_node_features, hidden_channels, num_classes,\n",
    "                           epochs, data_x_cpu, data_y_cpu, global_train_mask_cpu, global_val_mask_cpu, global_test_mask_cpu,\n",
    "                           global_edge_index_cpu):\n",
    "    \"\"\"\n",
    "    Function executed by each Dask worker process to train on its partition.\n",
    "    It takes the global data and masks to evaluate global accuracy.\n",
    "    \"\"\"\n",
    "    # Setup distributed environment modify \n",
    "  # Setup distributed environment\n",
    "# These environment variables are crucial for PyTorch's distributed training (e.g., DistributedDataParallel)\n",
    "# or Dask's distributed setup to allow workers to find and communicate with the master node.\n",
    "\n",
    "# MASTER_ADDR: Specifies the IP address of the master node (the rank 0 process)\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "# For single-node training (all GPUs on the same machine), 'localhost' is correct.\n",
    "#\n",
    "# For multi-node training on AWS GPU instances:\n",
    "# You MUST replace 'localhost' with the private IP address of the designated master instance.\n",
    "# Example: os.environ['MASTER_ADDR'] = '172.31.X.Y' (where 172.31.X.Y is the private IP of your master instance)\n",
    "# Ensure security groups allow traffic on MASTER_PORT between instances.\n",
    "\n",
    "# MASTER_PORT: Specifies the port for communication between the master and worker nodes.\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "# This port must be open and accessible between all instances in your distributed setup.\n",
    "# '12355' is a common choice, but any unused port can be used.\n",
    "# For AWS instances, ensure this port is explicitly allowed in the security group rules\n",
    "# for both inbound and outbound traffic among your instances. \n",
    "\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size) \n",
    "\n",
    "    device = torch.device(f'cuda:{rank}') \n",
    "\n",
    "    print(f\"Worker {rank}: Starting training on its partition on device {device}...\")\n",
    "    model = GCN(num_node_features, hidden_channels, num_classes).to(device)\n",
    "    model = DDP(model, device_ids=[rank]) \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Move global data to device once\n",
    "    data_x_gpu = data_x_cpu.to(device)\n",
    "    data_y_gpu = data_y_cpu.to(device)\n",
    "    global_train_mask_gpu = global_train_mask_cpu.to(device)\n",
    "    global_val_mask_gpu = global_val_mask_cpu.to(device)\n",
    "    global_test_mask_gpu = global_test_mask_cpu.to(device)\n",
    "    global_edge_index_gpu = global_edge_index_cpu.to(device)\n",
    "\n",
    "    # Get partition specific data for this rank\n",
    "    partition_data = partition_data_list[rank]\n",
    "    partition_node_mask_gpu = partition_data['node_mask'].to(device)\n",
    "    \n",
    "    # Filter global masks to only include nodes within this partition for local loss calculation\n",
    "    local_train_mask = global_train_mask_gpu & partition_node_mask_gpu\n",
    "\n",
    "    # --- PyTorch Profiler Integration for Distributed Training ---\n",
    "    # Only profile on rank 0 to avoid redundant traces and simplify analysis\n",
    "    if rank == 0:\n",
    "        with torch.profiler.profile(\n",
    "            schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
    "            on_trace_ready=torch.profiler.tensorboard_trace_handler(os.path.join(LOG_DIR, f\"distributed_gcn_rank{rank}\")),\n",
    "            with_stack=True\n",
    "        ) as prof:\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                out = model(data_x_gpu, global_edge_index_gpu)\n",
    "                \n",
    "                if local_train_mask.sum() > 0:\n",
    "                    loss = criterion(out[local_train_mask], data_y_gpu[local_train_mask])\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    loss = torch.tensor(0.0).to(device)\n",
    "\n",
    "                dist.all_reduce(loss, op=dist.ReduceOp.SUM)\n",
    "                loss_avg = loss.item() / world_size\n",
    "\n",
    "                if (epoch % 10 == 0 or epoch == 1):\n",
    "                    train_acc = evaluate_global(model.module, data_x_gpu, global_edge_index_gpu, data_y_gpu, global_train_mask_gpu)\n",
    "                    val_acc = evaluate_global(model.module, data_x_gpu, global_edge_index_gpu, data_y_gpu, global_val_mask_gpu)\n",
    "                    print(f'Worker {rank} (Aggregator) - Epoch: {epoch:03d}, Local Loss: {loss_avg:.4f}, Global Train Acc: {train_acc:.4f}, Global Val Acc: {val_acc:.4f}')\n",
    "                \n",
    "                prof.step() # Mark the end of the step for the profiler\n",
    "                dist.barrier() # Ensure all processes are synchronized before moving to next epoch\n",
    "    else: # For other ranks, just run the training loop without profiling\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(data_x_gpu, global_edge_index_gpu)\n",
    "            \n",
    "            if local_train_mask.sum() > 0:\n",
    "                loss = criterion(out[local_train_mask], data_y_gpu[local_train_mask])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                loss = torch.tensor(0.0).to(device)\n",
    "\n",
    "            dist.all_reduce(loss, op=dist.ReduceOp.SUM)\n",
    "            # No print for other ranks to avoid cluttered output\n",
    "            \n",
    "            dist.barrier()\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "    print(f\"Worker {rank}: Finished training and destroying process group.\")\n",
    "    \n",
    "    if rank == 0:\n",
    "        return model.module.state_dict()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def evaluate_global(model, x, edge_index, y, mask):\n",
    "    \"\"\"Evaluates accuracy on a global mask. Assumes model and data are already on the correct device.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(x, edge_index)\n",
    "        pred = out[mask].argmax(dim=1)\n",
    "        correct = (pred == y[mask]).sum()\n",
    "        acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "# --- 5. Main Distributed Training Orchestrator with Dask ---\n",
    "def run_distributed_training(num_partitions, partition_type='random', epochs=200):\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"No GPUs available. Distributed training across GPUs cannot be performed.\")\n",
    "        return 0.0, 0.0, None # Return dummy values and None for model state\n",
    "\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_partitions > num_gpus:\n",
    "        print(f\"Warning: Requested {num_partitions} partitions but only {num_gpus} GPUs available. Using {num_gpus} GPUs.\")\n",
    "        num_partitions = num_gpus\n",
    "    \n",
    "    if num_partitions == 0:\n",
    "        print(\"No GPUs available to run distributed training.\")\n",
    "        return 0.0, 0.0, None\n",
    "\n",
    "    print(f\"\\n--- Running Distributed Training with {num_partitions} Partitions ({partition_type}) across GPUs (Dask Orchestrated) ---\")\n",
    "    \n",
    "    partition_start_time = time.time()\n",
    "    if partition_type == 'random':\n",
    "        partitions = random_partition_graph(data, num_partitions)\n",
    "    elif partition_type == 'metis':\n",
    "        partitions = metis_partition_graph(data, num_partitions)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid partition_type. Choose 'random' or 'metis'.\")\n",
    "    partition_end_time = time.time()\n",
    "    print(f\"Total partitioning time: {partition_end_time - partition_start_time:.4f} seconds.\")\n",
    "\n",
    "    total_nodes_sum = 0\n",
    "    total_cut_edges_sum = 0\n",
    "    print(\"\\nPartition Statistics:\")\n",
    "    for i, p in enumerate(partitions):\n",
    "        print(f\"  Partition {i}: Nodes = {p['num_nodes_in_partition']}, Cut Edges = {p['num_cut_edges']}\")\n",
    "        total_nodes_sum += p['num_nodes_in_partition']\n",
    "        total_cut_edges_sum += p['num_cut_edges']\n",
    "    print(f\"  Total nodes across partitions (sum): {total_nodes_sum}\")\n",
    "    print(f\"  Total cut edges across partitions (sum of individual partition's cuts): {total_cut_edges_sum}\")\n",
    "    print(f\"  Note: Total cut edges sum is 2x the actual unique cut edges as each is counted from both sides.\")\n",
    "\n",
    "    cluster = LocalCluster(n_workers=num_partitions, threads_per_worker=1, processes=True)\n",
    "    client = Client(cluster)\n",
    "    print(f\"Dask Dashboard: {client.dashboard_link}\")\n",
    "\n",
    "    common_args = (partitions, data.num_node_features, hidden_channels, dataset.num_classes,\n",
    "                   epochs, data.x, data.y, data.train_mask, data.val_mask, data.test_mask, data.edge_index)\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    futures = []\n",
    "    for i in range(num_partitions):\n",
    "        futures.append(client.submit(train_partition_worker, i, num_partitions, *common_args))\n",
    "\n",
    "    results_list = client.gather(futures)\n",
    "    \n",
    "    final_model_state_dict = None\n",
    "    for res in results_list:\n",
    "        if res is not None:\n",
    "            final_model_state_dict = res\n",
    "            break\n",
    "    \n",
    "    if final_model_state_dict is None:\n",
    "        print(\"Error: Could not retrieve final model state from any worker.\")\n",
    "        return 0.0, 0.0, None\n",
    "\n",
    "    training_end_time = time.time()\n",
    "    total_training_time = training_end_time - training_start_time\n",
    "    print(f\"\\nTotal Distributed Training Time ({num_partitions} partitions): {total_training_time:.4f} seconds.\")\n",
    "\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "    print(\"Dask client and cluster closed.\")\n",
    "\n",
    "    global_model = GCN(data.num_node_features, hidden_channels, dataset.num_classes)\n",
    "    global_model.load_state_dict(final_model_state_dict)\n",
    "\n",
    "    test_acc = evaluate_global(global_model, data.x, data.edge_index, data.y, data.test_mask)\n",
    "    print(f'Final Test Accuracy (Aggregated Model): {test_acc:.4f}')\n",
    "    return total_training_time, test_acc, global_model # Return the trained model for clustering\n",
    "\n",
    "# # --- New Function: Random Walk based Node Classification (Node2Vec) ---\n",
    "# def run_node2vec_classification(data, embedding_dim=128, walk_length=20, context_size=10,\n",
    "#                                 walks_per_node=10, num_negative_samples=1, batch_size=128,\n",
    "#                                 n2v_epochs=50, lr=0.01, num_classes=None):\n",
    "#     \"\"\"\n",
    "#     Runs Node2Vec to generate embeddings and then trains a Logistic Regression\n",
    "#     classifier on these embeddings for node classification.\n",
    "#     \"\"\"\n",
    "#     if not NODE2VEC_AVAILABLE:\n",
    "#         print(\"\\nNode2Vec is not available. Skipping Node2Vec based Node Classification.\")\n",
    "#         return 0.0, 0.0 # Return dummy values\n",
    "        \n",
    "#     print(\"\\n--- Running Node2Vec based Node Classification ---\")\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     device = 'cpu' # Node2Vec training is often done on CPU or can be moved to GPU if desired\n",
    "    \n",
    "#     model_n2v = Node2Vec(data.edge_index, embedding_dim=embedding_dim, walk_length=walk_length,\n",
    "#                          context_size=context_size, walks_per_node=walks_per_node,\n",
    "#                          num_negative_samples=num_negative_samples, p=1.0, q=1.0, sparse=True).to(device)\n",
    "    \n",
    "#     optimizer_n2v = torch.optim.Adam(model_n2v.parameters(), lr=lr)\n",
    "\n",
    "#     loader = model_n2v.loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "#     print(f\"Training Node2Vec embeddings for {n2v_epochs} epochs...\")\n",
    "#     # --- PyTorch Profiler Integration for Node2Vec (Optional) ---\n",
    "#     # For Node2Vec, we can also add a profiler.\n",
    "#     # The `on_trace_ready` handler will save the trace to a file.\n",
    "#     with torch.profiler.profile(\n",
    "#         schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
    "#         on_trace_ready=torch.profiler.tensorboard_trace_handler(os.path.join(LOG_DIR, \"node2vec_profiler\")),\n",
    "#         with_stack=True\n",
    "#     ) as prof:\n",
    "#         for epoch in range(1, n2v_epochs + 1):\n",
    "#             model_n2v.train()\n",
    "#             total_loss = 0\n",
    "#             for pos_rw, neg_rw in loader:\n",
    "#                 optimizer_n2v.zero_grad()\n",
    "#                 loss = model_n2v.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "#                 loss.backward()\n",
    "#                 optimizer_n2v.step()\n",
    "#                 total_loss += loss.item()\n",
    "#             if epoch % 10 == 0 or epoch == 1:\n",
    "#                 print(f'Node2Vec Epoch: {epoch:03d}, Loss: {total_loss / len(loader):.4f}')\n",
    "#             prof.step() # Mark the end of the step for the profiler\n",
    "    \n",
    "#     model_n2v.eval()\n",
    "#     with torch.no_grad():\n",
    "#         embeddings = model_n2v(torch.arange(data.num_nodes, device=device)).cpu().numpy()\n",
    "\n",
    "#     embedding_time = time.time() - start_time\n",
    "#     print(f\"Node2Vec embedding generation complete in {embedding_time:.4f} seconds.\")\n",
    "\n",
    "#     print(\"Training Logistic Regression classifier on Node2Vec embeddings...\")\n",
    "#     train_indices = data.train_mask.nonzero(as_tuple=True)[0].numpy()\n",
    "#     test_indices = data.test_mask.nonzero(as_tuple=True)[0].numpy()\n",
    "\n",
    "#     X_train = embeddings[train_indices]\n",
    "#     y_train = data.y[train_indices].numpy()\n",
    "#     X_test = embeddings[test_indices]\n",
    "#     y_test = data.y[test_indices].numpy()\n",
    "\n",
    "#     classifier = LogisticRegression(solver='liblinear', multi_class='auto', max_iter=200)\n",
    "#     classifier.fit(X_train, y_train)\n",
    "\n",
    "#     y_pred = classifier.predict(X_test)\n",
    "#     test_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#     classification_time = time.time() - (start_time + embedding_time)\n",
    "#     total_time = embedding_time + classification_time\n",
    "\n",
    "#     print(f\"Logistic Regression classification complete in {classification_time:.4f} seconds.\")\n",
    "#     print(f\"Node2Vec + LR Total Time: {total_time:.4f} seconds, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "#     return total_time, test_acc\n",
    "\n",
    "# --- New Function: GCN-based Clustering ---\n",
    "# def run_gcn_clustering(trained_gcn_model, data_x_cpu, data_edge_index_cpu, data_y_cpu, num_classes):\n",
    "#     \"\"\"\n",
    "#     Extracts embeddings from a trained GCN model and performs K-Means clustering.\n",
    "#     Evaluates clustering quality using Adjusted Rand Index and Normalized Mutual Information.\n",
    "#     \"\"\"\n",
    "#     print(\"\\n--- Running GCN-based Clustering ---\")\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#     trained_gcn_model.eval() # Set model to evaluation mode\n",
    "#     trained_gcn_model.to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         # Get embeddings from the hidden layer of the GCN\n",
    "#         # We pass return_embeddings=True to get the output of conv1 (hidden layer)\n",
    "#         gcn_embeddings = trained_gcn_model(data_x_cpu.to(device), data_edge_index_cpu.to(device), return_embeddings=True).cpu().numpy()\n",
    "\n",
    "#     embedding_extraction_time = time.time() - start_time\n",
    "#     print(f\"GCN embedding extraction complete in {embedding_extraction_time:.4f} seconds.\")\n",
    "\n",
    "#     # Perform K-Means clustering\n",
    "#     print(f\"Performing K-Means clustering with {num_classes} clusters...\")\n",
    "#     kmeans = KMeans(n_clusters=num_classes, random_state=0, n_init=10) # n_init for robustness\n",
    "#     cluster_labels = kmeans.fit_predict(gcn_embeddings)\n",
    "\n",
    "#     clustering_time = time.time() - (start_time + embedding_extraction_time)\n",
    "#     total_time = embedding_extraction_time + clustering_time\n",
    "\n",
    "#     # Evaluate clustering quality using ground-truth labels (data.y)\n",
    "#     # Note: Clustering is unsupervised, so \"accuracy\" is not directly applicable.\n",
    "#     # We use metrics that compare the discovered clusters to the true classes.\n",
    "#     true_labels = data_y_cpu.numpy()\n",
    "\n",
    "#     ari_score = adjusted_rand_score(true_labels, cluster_labels)\n",
    "#     nmi_score = normalized_mutual_info_score(true_labels, cluster_labels)\n",
    "\n",
    "#     print(f\"K-Means clustering complete in {clustering_time:.4f} seconds.\")\n",
    "#     print(f\"GCN Clustering Total Time: {total_time:.4f} seconds.\")\n",
    "#     print(f\"Adjusted Rand Index (ARI): {ari_score:.4f}\")\n",
    "#     print(f\"Normalized Mutual Information (NMI): {nmi_score:.4f}\")\n",
    "\n",
    "#     return total_time, ari_score, nmi_score # Return ARI as the primary \"accuracy\" for plotting\n",
    "\n",
    "\n",
    "# --- 6. Main Execution Block for Experimentation and Profiling ---\n",
    "if __name__ == '__main__':\n",
    "    hidden_channels = 16 # Number of channels in the GCN hidden layer\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        num_partitions_list = [1, 2,3,4,8] \n",
    "        num_gpus_available = torch.cuda.device_count()\n",
    "        num_partitions_list = [p for p in num_partitions_list if p <= num_gpus_available]\n",
    "        if not num_partitions_list:\n",
    "            print(\"No suitable number of partitions to run multi-GPU training (0 or more partitions than GPUs available).\")\n",
    "            print(\"Running only sequential CPU training.\")\n",
    "            num_partitions_list = [1]\n",
    "    else:\n",
    "        print(\"No GPUs available. Running only sequential CPU training.\")\n",
    "        num_partitions_list = [1]\n",
    "\n",
    "    results = {} # Stores {'config_name': {'time': ..., 'accuracy': ..., 'nmi': ...}}\n",
    "\n",
    "    # --- Run Sequential GCN (1 partition) as Baseline ---\n",
    "    print(\"\\n--- Running Sequential GCN Training (1 Partition) as Baseline ---\")\n",
    "    # Using PyTorch profiler for sequential GCN training\n",
    "    with torch.profiler.profile(\n",
    "        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(os.path.join(LOG_DIR, \"sequential_gcn_profiler\")),\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "        model_sequential = GCN(data.num_node_features, hidden_channels, dataset.num_classes)\n",
    "        optimizer_sequential = torch.optim.Adam(model_sequential.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "        criterion_sequential = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        device_sequential = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        model_sequential.to(device_sequential)\n",
    "        data_x_seq = data.x.to(device_sequential)\n",
    "        data_y_seq = data.y.to(device_sequential)\n",
    "        data_edge_index_seq = data.edge_index.to(device_sequential)\n",
    "        data_train_mask_seq = data.train_mask.to(device_sequential)\n",
    "        data_val_mask_seq = data.val_mask.to(device_sequential)\n",
    "        data_test_mask_seq = data.test_mask.to(device_sequential)\n",
    "\n",
    "        seq_start_time = time.time()\n",
    "        for epoch in range(1, 101): # Fewer epochs for sequential for quicker profiling\n",
    "            model_sequential.train()\n",
    "            optimizer_sequential.zero_grad()\n",
    "            out = model_sequential(data_x_seq, data_edge_index_seq)\n",
    "            loss = criterion_sequential(out[data_train_mask_seq], data_y_seq[data_train_mask_seq])\n",
    "            loss.backward()\n",
    "            optimizer_sequential.step()\n",
    "            if epoch % 10 == 0 or epoch == 1:\n",
    "                train_acc = evaluate_global(model_sequential, data_x_seq, data_edge_index_seq, data_y_seq, data_train_mask_seq)\n",
    "                val_acc = evaluate_global(model_sequential, data_x_seq, data_edge_index_seq, data_y_seq, data_val_mask_seq)\n",
    "                print(f'Sequential GCN - Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "            prof.step() # Mark the end of the step for the profiler\n",
    "        seq_end_time = time.time()\n",
    "        seq_total_time = seq_end_time - seq_start_time\n",
    "        seq_test_acc = evaluate_global(model_sequential, data_x_seq, data_edge_index_seq, data_y_seq, data_test_mask_seq)\n",
    "        print(f\"Sequential GCN Training Time (1 partition): {seq_total_time:.4f} seconds.\")\n",
    "        print(f\"Sequential GCN Test Accuracy: {seq_test_acc:.4f}\")\n",
    "    \n",
    "    # No cProfile/pstats for GCN sequential training as torch.profiler is used\n",
    "    results['GCN_Sequential'] = {'time': seq_total_time, 'accuracy': seq_test_acc, 'nmi': None}\n",
    "\n",
    "\n",
    "    # --- Run Distributed GCN Training ---\n",
    "    trained_gcn_model_dist = None # To store the model for clustering\n",
    "    if any(p > 1 for p in num_partitions_list):\n",
    "        for n_parts in num_partitions_list:\n",
    "            if n_parts == 1:\n",
    "                continue\n",
    "            print(f\"\\n--- Running Distributed GCN Training with {n_parts} Partitions (Random Partitioning) ---\")\n",
    "            # Profiling for distributed training is handled inside train_partition_worker on rank 0\n",
    "            train_time, test_acc, model_for_clustering = run_distributed_training(n_parts, partition_type='random', epochs=100)\n",
    "            results[f'GCN_Random_{n_parts}_parts'] = {'time': train_time, 'accuracy': test_acc, 'nmi': None}\n",
    "            if trained_gcn_model_dist is None:\n",
    "                trained_gcn_model_dist = model_for_clustering\n",
    "\n",
    "        if METIS_AVAILABLE:\n",
    "            for n_parts in num_partitions_list:\n",
    "                if n_parts == 1:\n",
    "                    continue\n",
    "                print(f\"\\n--- Running Distributed GCN Training with {n_parts} Partitions (METIS Partitioning) ---\")\n",
    "                # Profiling for distributed training is handled inside train_partition_worker on rank 0\n",
    "                train_time, test_acc, model_for_clustering = run_distributed_training(n_parts, partition_type='metis', epochs=100)\n",
    "                results[f'GCN_METIS_{n_parts}_parts'] = {'time': train_time, 'accuracy': test_acc, 'nmi': None}\n",
    "                if trained_gcn_model_dist is None:\n",
    "                    trained_gcn_model_dist = model_for_clustering\n",
    "\n",
    "    # --- Run Node2Vec based Classification ---\n",
    "#     n2v_embedding_dim = 128\n",
    "#     n2v_walk_length = 20\n",
    "#     n2v_context_size = 10\n",
    "#     n2v_walks_per_node = 10\n",
    "#     n2v_num_negative_samples = 1\n",
    "#     n2v_batch_size = 128\n",
    "#     n2v_epochs = 50\n",
    "\n",
    "#     # Profiling for Node2Vec is integrated within run_node2vec_classification\n",
    "#     n2v_time, n2v_acc = run_node2vec_classification(\n",
    "#         data,\n",
    "#         embedding_dim=n2v_embedding_dim,\n",
    "#         walk_length=n2v_walk_length,\n",
    "#         context_size=n2v_context_size,\n",
    "#         walks_per_node=n2v_walks_per_node,\n",
    "#         num_negative_samples=n2v_num_negative_samples,\n",
    "#         batch_size=n2v_batch_size,\n",
    "#         n2v_epochs=n2v_epochs,\n",
    "#         num_classes=dataset.num_classes\n",
    "#     )\n",
    "#     results['Node2Vec_LR'] = {'time': n2v_time, 'accuracy': n2v_acc, 'nmi': None}\n",
    "\n",
    "\n",
    "    # --- Run GCN-based Clustering (using the sequential GCN model for simplicity) ---\n",
    "    # if model_sequential is not None:\n",
    "    #     gcn_cluster_time, gcn_cluster_ari, gcn_cluster_nmi = run_gcn_clustering(\n",
    "    #         model_sequential, data.x, data.edge_index, data.y, dataset.num_classes\n",
    "    #     )\n",
    "    #     results['GCN_Clustering'] = {'time': gcn_cluster_time, 'accuracy': gcn_cluster_ari, 'nmi': gcn_cluster_nmi}\n",
    "\n",
    "    print(\"\\n--- Summary of All Results ---\")\n",
    "    for config, res in results.items():\n",
    "        print(f\"Configuration: {config}, Total Time: {res['time']:.4f}s, Test Accuracy/ARI: {res['accuracy']:.4f}\", end=\"\")\n",
    "        if res['nmi'] is not None:\n",
    "            print(f\", NMI: {res['nmi']:.4f}\")\n",
    "        else:\n",
    "            print(\"\")\n",
    "\n",
    "    # --- 7. Visualization of Results ---\n",
    "    print(\"\\n--- Generating Performance Plots ---\")\n",
    "    configs = list(results.keys())\n",
    "    times = [results[c]['time'] for c in configs]\n",
    "    performance_scores = [results[c]['accuracy'] for c in configs]\n",
    "\n",
    "    # --- Colorblind-friendly palette ---\n",
    "    # Using a subset of the Color Brewer 'Set2' or 'Paired' for distinct categories\n",
    "    # Or a custom set based on common colorblind-safe recommendations\n",
    "    colorblind_colors = {\n",
    "        'GCN_Sequential': '#8DA0CB', # Light Blue/Lavender\n",
    "        'GCN_Random': '#FC8D62',     # Salmon/Orange\n",
    "        'GCN_METIS': '#66C2A5',      # Teal/Green\n",
    "        # Add more if other configurations are uncommented later\n",
    "    }\n",
    "\n",
    "    colors = []\n",
    "    for config in configs:\n",
    "        # Assign colors based on the main category in the config name\n",
    "        if 'GCN_Sequential' in config:\n",
    "            colors.append(colorblind_colors['GCN_Sequential'])\n",
    "        elif 'GCN_Random' in config:\n",
    "            colors.append(colorblind_colors['GCN_Random'])\n",
    "        elif 'GCN_METIS' in config:\n",
    "            colors.append(colorblind_colors['GCN_METIS'])\n",
    "        else:\n",
    "            colors.append('#A6D854') # A fallback color if new configs appear\n",
    "\n",
    "    # Plotting Total Times\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.bar(configs, times, color=colors)\n",
    "    plt.xlabel('Configuration', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Total Time (seconds)', fontsize=12, fontweight='bold')\n",
    "    plt.title('Comparison of Total Time Across Configurations', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10, fontweight='bold')\n",
    "    plt.yticks(fontsize=10, fontweight='bold') # Make y-tick labels bold\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting Performance Scores (Accuracy for Classification, ARI for Clustering)\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.bar(configs, performance_scores, color=colors)\n",
    "    plt.xlabel('Configuration', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Performance Score (Accuracy)', fontsize=12, fontweight='bold')\n",
    "    plt.title('Comparison of Performance Scores Across Configurations', fontsize=14, fontweight='bold')\n",
    "    plt.ylim(min(performance_scores) * 0.9, max(performance_scores) * 1.05)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10, fontweight='bold')\n",
    "    plt.yticks(fontsize=10, fontweight='bold') # Make y-tick labels bold\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Plotting Performance Scores (Accuracy for Classification, ARI for Clustering)\n",
    "    # plt.figure(figsize=(14, 7))\n",
    "    # plt.bar(configs, performance_scores, color=colors)\n",
    "    # plt.xlabel('Configuration')\n",
    "    # plt.ylabel('Performance Score (Accuracy for Classification, ARI for Clustering)')\n",
    "    # plt.title('Comparison of Performance Scores Across Configurations')\n",
    "    # plt.ylim(min(performance_scores) * 0.9, max(performance_scores) * 1.05)\n",
    "    # plt.xticks(rotation=45, ha='right')\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    # Plotting NMI Scores for Clustering (if applicable)\n",
    "    # if nmi_scores:\n",
    "    #     nmi_colors = [colors[configs.index(c)] for c in nmi_configs]\n",
    "    #     plt.figure(figsize=(8, 6))\n",
    "    #     plt.bar(nmi_configs, nmi_scores, color=nmi_colors)\n",
    "    #     plt.xlabel('Clustering Configuration')\n",
    "    #     plt.ylabel('Normalized Mutual Information (NMI)')\n",
    "    #     plt.title('Comparison of NMI Scores for Clustering Configurations')\n",
    "    #     plt.ylim(min(nmi_scores) * 0.9, max(nmi_scores) * 1.05)\n",
    "    #     plt.xticks(rotation=45, ha='right')\n",
    "    #     plt.tight_layout()\n",
    "    #     plt.show()\n",
    "\n",
    "    #print(\"\\nHPC GCN, Node2Vec, and GCN-based Clustering examples finished.\")\n",
    "    print(\"This comparison highlights different approaches to node classification and clustering on graphs.\")\n",
    "    print(\"For GCN Clustering, 'accuracy' in the plot refers to Adjusted Rand Index (ARI).\")\n",
    "    print(\"\\n--- PyTorch Profiler Trace Files ---\")\n",
    "    print(f\"Trace files have been saved to the '{LOG_DIR}' directory.\")\n",
    "    print(\"You can visualize these traces using TensorBoard by running:\")\n",
    "    print(f\"tensorboard --logdir {LOG_DIR}\")\n",
    "    print(\"Then open your web browser and navigate to the address provided by TensorBoard (usually http://localhost:6006).\")\n",
    "    print(\"In TensorBoard, go to the 'Profile' tab to explore the execution details.\")\n",
    "    print(\"Further optimizations and hyperparameter tuning would be beneficial for all methods.\")\n",
    "    \n",
    "    print(\"\\n--- NVIDIA Nsight Systems Profiling ---\")\n",
    "    print(\"NVIDIA Nsight Systems is an external command-line profiler that provides deep insights into GPU utilization, CUDA kernel performance, and CPU-GPU interactions.\")\n",
    "    print(\"To profile your script with Nsight Systems, ensure it is installed and in your system's PATH.\")\n",
    "    print(\"Then, run your Python script from the terminal using the `nsys` command. For example:\")\n",
    "    print(f\"nsys profile -o {LOG_DIR}/nsys_report --stats=true python your_script_name.py\")\n",
    "    print(\"Replace 'your_script_name.py' with the actual name of your Python script.\")\n",
    "    print(\"This will generate an Nsight Systems report (`.qdrep` file) in the specified directory.\")\n",
    "    print(\"You can then open this report using the Nsight Systems GUI for detailed analysis.\")\n",
    "    print(\"Further optimizations and hyperparameter tuning would be beneficial for all methods.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a171d20-1aa5-4380-ade5-ee322103711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/hpc_gcn_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e7a6f-4bed-41b2-a76d-01f6760c05f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch GPU 2.5 (py312)",
   "language": "python",
   "name": "pytorch-gpu-2.5-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
